{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbwang2016/normalizing_flow_tests/blob/main/coulomb_gas_realnvp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "CRkl2RYH_4d_"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax.config import config   \n",
        "config.update(\"jax_enable_x64\", True)\n",
        "import jax.numpy as jnp\n",
        "from jax import lax\n",
        "from jax.experimental import ode \n",
        "from jax.experimental import stax\n",
        "from jax.experimental import optimizers\n",
        "from jax.experimental.stax import Dense, Flatten, Softplus, Relu\n",
        "from jax.nn.initializers import normal\n",
        "import math \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFGH-FFECpwr",
        "outputId": "14a0358e-2a73-41d4-8a6d-954c534decf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar  3 08:36:23 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    76W / 149W |  10368MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[GpuDevice(id=0, process_index=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOP-tgCDWGhp"
      },
      "source": [
        "We'd like to study thermodynamic property of the classcial Coulomb gas, whose Hamiltonian reads\n",
        "\n",
        "$$H= \\sum_{i<j} \\frac{1}{|\\boldsymbol{x}_i - \\boldsymbol{x}_j|} + \\sum_i  \\frac{\\boldsymbol{x}_i^2 }{2}. $$\n",
        "The second term is a harmonic trapping potential. It makes our story easier (no need to consider periodic bondary condition or Ewald sum for long range interaction.)\n",
        "\n",
        "The way to go is to minimize the variationial free energy with respect to a variational probability density $p(\\boldsymbol{x})$\n",
        "\n",
        "$$\\mathcal{L} = \\mathbb{E}_{\\boldsymbol{x} \\sim p(\\boldsymbol{x})} \\left [ \\ln p(\\boldsymbol{x}) + \\beta H(\\boldsymbol{x}) \\right] \\ge -\\ln Z, $$ \n",
        "where $Z = \\int d \\boldsymbol{x} e^{-\\beta H}$ and $\\beta$ is the inverse temperature. The equality holds when $p(\\boldsymbol{x}) = e^{-\\beta H}/Z$, i.e., we achieve the exact solution. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caqRMIF0YjHl"
      },
      "source": [
        "First thing first, here is the energy function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "lLXmsJGN7UeO"
      },
      "outputs": [],
      "source": [
        "def energy(x, n, dim):\n",
        "    i, j = np.triu_indices(n, k=1)\n",
        "    r_ee = jnp.linalg.norm((jnp.reshape(x, (n, 1, dim)) - jnp.reshape(x, (1, n, dim)))[i,j], axis=-1) \n",
        "    v_ee = jnp.sum(1/r_ee)\n",
        "    return jnp.sum(x**2/2) + v_ee "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obkF0f8_YQLk"
      },
      "source": [
        "The probablistic model we are going to use is the continuous normaliznig flow https://arxiv.org/abs/1806.07366. This is not going to be the most effecient approach. But it is certainly fun! \n",
        "\n",
        "In a nutshell, we integrate these two equations to transform samples and their log-likelihoods:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\frac{d \\boldsymbol{x} }{d t } & =   \\boldsymbol{v}, \\tag{1}  \\\\\n",
        "\\frac{d \\ln p (\\boldsymbol{x}) }{d t} &  = -\\nabla \\cdot \\boldsymbol{v} \\tag{2}. \n",
        "\\end{eqnarray}\n",
        "\n",
        "Here $\\boldsymbol{v}$ is a velocity field we parametrize using a neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "XH9axBgPAMWX"
      },
      "outputs": [],
      "source": [
        "def nvp_forward(net_params, shift_and_log_scale_fn, x, flip=False):\n",
        "    d = x.shape[-1]//2\n",
        "    x1, x2 = x[:, :d], x[:, d:]\n",
        "    if flip:\n",
        "        x2, x1 = x1, x2\n",
        "    shift, log_scale = shift_and_log_scale_fn(net_params, x1)\n",
        "    y2 = x2*jnp.exp(log_scale) + shift\n",
        "    if flip:\n",
        "        x1, y2 = y2, x1\n",
        "    y = jnp.concatenate([x1, y2], axis=-1)\n",
        "    return y\n",
        "\n",
        "def nvp_inverse(net_params, shift_and_log_scale_fn, y, flip=False):\n",
        "    d = y.shape[-1]//2\n",
        "    y1, y2 = y[:, :d], y[:, d:]\n",
        "    if flip:\n",
        "        y1, y2 = y2, y1\n",
        "    shift, log_scale = shift_and_log_scale_fn(net_params, y1)\n",
        "    x2 = (y2-shift)*jnp.exp(-log_scale)\n",
        "    if flip:\n",
        "        y1, x2 = x2, y1\n",
        "    x = jnp.concatenate([y1, x2], axis=-1)\n",
        "    return x, log_scale\n",
        "\n",
        "\n",
        "def flow(ps, configs, x):\n",
        "    '''\n",
        "    flow from z to x\n",
        "    '''\n",
        "    for p, config in zip(ps, configs):\n",
        "        shift_log_scale_fn, flip = config\n",
        "        x = nvp_forward(p, shift_log_scale_fn, x, flip=flip)\n",
        "    return x\n",
        "\n",
        "def logp(ps, configs, y):\n",
        "    '''\n",
        "    likelihood of given samples\n",
        "    '''\n",
        "    \n",
        "    def base_logp(x):\n",
        "        return -0.5 * jnp.sum( x**2 + jnp.log(2 * math.pi), axis=-1)\n",
        "\n",
        "    rtn = 0\n",
        "    for p, config in list(zip(ps, configs))[::-1]:\n",
        "        shift_log_scale_fn, flip = config\n",
        "        y, log_scale = nvp_inverse(p, shift_log_scale_fn, y, flip=flip)\n",
        "        rtn -= jnp.sum(log_scale, axis=-1)\n",
        "    rtn += base_logp(y)\n",
        "    \n",
        "    return rtn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8lBKsQFY9Sd"
      },
      "source": [
        "Gradient of the objective function is\n",
        "\n",
        "$$\\nabla \\mathcal{L} = \\mathbb{E}_{\\boldsymbol{x} \\sim p(\\boldsymbol{x})} \\left[ f (\\boldsymbol{x})  \\nabla \\ln p(\\boldsymbol{x}) \\right],$$ \n",
        "where $f (\\boldsymbol{x}) = \\ln p(\\boldsymbol{x}) + \\beta H(\\boldsymbol{x})$. This is known as the REINFORCE algorithm, or score function gradient estimator. See https://arxiv.org/abs/1906.10652 for more details. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "bH9bpC1l0Wnd"
      },
      "outputs": [],
      "source": [
        "def make_reinforce_loss(cs, n, dim, beta):\n",
        "\n",
        "    batch_energy = jax.vmap(energy, (0, None, None), 0) \n",
        "    # batch_logp = jax.vmap(logp, (None, None, 0), 0) \n",
        "\n",
        "    def loss(params, x):\n",
        "    \n",
        "        entropy = logp(params, cs, x)\n",
        "\n",
        "        f = entropy + beta*batch_energy(x, n, dim) \n",
        "        f = jax.lax.stop_gradient(f)\n",
        "\n",
        "        f_mean = jnp.mean(f)\n",
        "\n",
        "        f_std = jnp.std(f)/jnp.sqrt(f.shape[0])\n",
        "\n",
        "        return jnp.mean((f - f_mean) * entropy), (f_mean, f_std)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6mFW4i_d-ec"
      },
      "source": [
        "Now we are ready to initialize a network for the velocity field $\\boldsymbol{v}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "kQ8b1xhJOquX"
      },
      "outputs": [],
      "source": [
        "batchsize = 8192\n",
        "n = 6\n",
        "dim = 2 \n",
        "beta = 5.0\n",
        "key = jax.random.PRNGKey(42)\n",
        "key, subkey = jax.random.split(key, 2)\n",
        "\n",
        "def init_nvp(D):\n",
        "    net_init, net_apply = stax.serial(\n",
        "        Dense(2048), Relu, Dense(2048), Relu, Dense(D))\n",
        "    in_shape = (-1, D//2)\n",
        "    out_shape, net_params = net_init(subkey, in_shape)\n",
        "    # print(out_shape)\n",
        "    def shift_and_log_scale_fn(net_params, x1):\n",
        "        s = net_apply(net_params, x1)\n",
        "        return np.split(s, 2, axis=1)\n",
        "    return net_params, shift_and_log_scale_fn\n",
        "\n",
        "def init_nvp_chain(D=6, n=8):\n",
        "    flip = False\n",
        "    ps, configs = [], []\n",
        "    for i in range(n):\n",
        "        p, f = init_nvp(D)\n",
        "        ps.append(p), configs.append((f, flip))\n",
        "        flip = not flip\n",
        "    return ps, configs\n",
        "\n",
        "\n",
        "ps0, cs = init_nvp_chain(n * dim)\n",
        "\n",
        "loss = make_reinforce_loss(cs, n, dim, beta)\n",
        "value_and_grad = jax.value_and_grad(loss, argnums=0, has_aux=True)\n",
        "\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size=1e-4)\n",
        "opt_state = opt_init(ps0)\n",
        "\n",
        "# in_shape = (-1, n*dim)\n",
        "# key, subkey = jax.random.split(key, 2)\n",
        "# out_shape, params = net_init(subkey, in_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaqJltIXsfpP"
      },
      "source": [
        "Here is the training loop. During training we monitor the density and loss histroy. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ps, cs = init_nvp_chain(n * dim)\n",
        "# x = jax.random.normal(subkey, (batchsize, n*dim)) \n",
        "# y = flow(ps, cs, x)\n",
        "# def tmp(ps, configs, y):\n",
        "#     for p, config in list(zip(ps, configs))[::-1]:\n",
        "#         shift_log_scale_fn, flip = config\n",
        "#         y, log_scale = nvp_inverse(p, shift_log_scale_fn, y, flip=flip)\n",
        "    \n",
        "#     return y\n",
        "# print(np.max(np.abs(np.array(x - tmp(ps, cs, y)))))\n",
        "# # print(ps)"
      ],
      "metadata": {
        "id": "wlacUc5_k2yN"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "mT0DuOUZAJPy",
        "outputId": "b0fcc77e-6258-4e8d-aea1-6ea410d46787"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def step(i, subkey, opt_state):\n",
        "    params = get_params(opt_state)\n",
        "    \n",
        "    x = jax.random.normal(subkey, (batchsize, n*dim)) \n",
        "    # x = jax.vmap(flow, (None, None, 0), 0)(params, network, x)\n",
        "    x = flow(params, cs, x)\n",
        "\n",
        "    x = jax.lax.stop_gradient(x)\n",
        "\n",
        "    value, grad = value_and_grad(params, x)\n",
        "\n",
        "    opt_state = opt_update(i, grad, opt_state)\n",
        "    return value, opt_state, x\n",
        "\n",
        "loss_histroy = []\n",
        "for i in range(500):\n",
        "    # key, subkey = jax.random.split(key)\n",
        "    value, opt_state, x = step(i, subkey, opt_state)\n",
        "    _, (f_mean, f_err) = value\n",
        "\n",
        "    loss_histroy.append([f_mean, f_err])\n",
        "    print([u[0] for u in loss_histroy])\n",
        "   \n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "    #v = network(get_params(opt_state), x[0]) # velocity on the first sample\n",
        "    #v = jnp.reshape(v, (n, dim))\n",
        "    x = jnp.reshape(x, (batchsize*n, dim)) \n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    #plt.plot(x[:, 0], x[:, 1],'o', alpha=0.5)\n",
        "    #plt.quiver(x[:, 0], x[:, 1], v[:, 0], v[:, 1])\n",
        "\n",
        "    #density plot\n",
        "    H, xedges, yedges = np.histogram2d(x[:, 0], x[:, 1], \n",
        "                                       bins=100, \n",
        "                                       range=((-4, 4), (-4, 4)),\n",
        "                        density=True)\n",
        "    plt.imshow(H, interpolation=\"nearest\", \n",
        "               extent=(xedges[0], xedges[-1], yedges[0], yedges[-1]),\n",
        "               cmap=\"inferno\")\n",
        "\n",
        "    plt.xlim([-4, 4])\n",
        "    plt.ylim([-4, 4])\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    y = np.reshape(np.array(loss_histroy), (-1, 2))\n",
        "    plt.errorbar(np.arange(i+1), y[:, 0], yerr=y[:, 1], marker='o', capsize=8)\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('variational free energy')\n",
        "    plt.pause(0.01)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV1vKMbaft74"
      },
      "source": [
        "Do you see structure emerges from training? \n",
        "Yes! It is called [Wigner molecule](https://en.wikipedia.org/wiki/Wigner_crystal). Just think about a few marbles in a bow. You got it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZmkaMMJeWVf"
      },
      "source": [
        "A few things to improve from this mininist implementation: \n",
        "* It seems the training breaks rotational invariance. But it shouldn't. How to incorporate that into the model design ? \n",
        "\n",
        "* How about permutation equivariance $\\mathcal{P} \\boldsymbol{v}(\\boldsymbol{x}) = v(\\mathcal{P} \\boldsymbol{x})$ ?\n",
        "\n",
        "* Do you feel it is a bit ackward to integrate the equation twice (one for sampling, one for the likelihood). It is actually easier to implement \"reparametrization\" gradient estimator. Code it and see why we were not using it here. \n",
        "\n",
        "* This code is painfully slow for such a tiny problem. This is mainly because we have repeatedly computed divergence $\\nabla \\cdot \\boldsymbol{v}$ of the neural network. See https://arxiv.org/abs/1810.01367 and https://arxiv.org/abs/1912.03579 for ideas of speeding up.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqnKhNB7aaTX"
      },
      "source": [
        "# sandbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HPfsGIybNr2"
      },
      "outputs": [],
      "source": [
        "def divergence_scan_jvp(f):\n",
        "    def _div_f(params, x):\n",
        "        n = x.shape[0]\n",
        "        eye = jnp.eye(n)\n",
        "\n",
        "        def _body_fun(val, i):\n",
        "            primal, tangent = jax.jvp(lambda x: f(params, x), (x,), (eye[i],))\n",
        "            return val + tangent[i], None\n",
        "                                             \n",
        "        return lax.scan(_body_fun, 0.0, jnp.arange(0, n))[0]\n",
        "    return _div_f\n",
        "\n",
        "def divergence_scan_vjp(f):\n",
        "    def _div_f(params, x):\n",
        "        n = x.shape[0]\n",
        "        eye = jnp.eye(n)\n",
        "\n",
        "        def _body_fun(val, i):\n",
        "            primal, vjp = jax.vjp(lambda x: f(params, x), x)\n",
        "            return val + vjp(eye[i])[0][i], None\n",
        "                                             \n",
        "        return lax.scan(_body_fun, 0.0, jnp.arange(0, n))[0]\n",
        "    return _div_f\n",
        "\n",
        "def divergence_rev(f):\n",
        "    def _div_f(params, x):\n",
        "        jac = jax.jacrev(lambda x: f(params, x))\n",
        "        return jnp.trace(jac(x))\n",
        "\n",
        "    return _div_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut_0LzrNbRNX"
      },
      "outputs": [],
      "source": [
        "def f(_, x):\n",
        "    return x**2+x\n",
        "\n",
        "x = jnp.ones(100)\n",
        "%timeit divergence_scan_jvp(f)(None, x).block_until_ready() \n",
        "%timeit divergence_scan_vjp(f)(None, x).block_until_ready() \n",
        "%timeit divergence_fwd(f)(None, x).block_until_ready()  \n",
        "%timeit divergence_rev(f)(None, x).block_until_ready()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyrQ-yiXNcoy"
      },
      "outputs": [],
      "source": [
        "%timeit jax.vmap(divergence_scan(network), (None, 0), 0)(params, x).block_until_ready() \n",
        "%timeit jax.vmap(divergence_fwd(network), (None, 0), 0)(params, x).block_until_ready() \n",
        "%timeit jax.vmap(divergence_rev(network), (None, 0), 0)(params, x).block_until_ready() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNJ86BpyTSJr"
      },
      "outputs": [],
      "source": [
        "from jax.scipy.stats import norm\n",
        "def logp1(x):\n",
        "    return -0.5 * jnp.sum( x**2 + jnp.log(2 * math.pi), axis=-1)\n",
        "def logp2(x):\n",
        "    return jnp.sum(norm.logpdf(x), axis=-1)\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, (4, 2))\n",
        "jnp.allclose(logp1(x), logp2(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1Szc8laqsIV"
      },
      "outputs": [],
      "source": [
        "%timeit jax.scipy.linalg.solve(A, b).block_until_ready()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVnPE21bqn81"
      },
      "outputs": [],
      "source": [
        "%timeit jax.scipy.sparse.linalg.gmres(Ax, b, tol=1e-6)[0].block_until_ready()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhq0AJU814ba"
      },
      "source": [
        "# New test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZnYsR6y88ja"
      },
      "outputs": [],
      "source": [
        "\n",
        "np.split(np.array([1,2,3,4]),2,axis=0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of coulomb_gas.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}